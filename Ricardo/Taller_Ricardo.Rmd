---
title: "Taller_Ricardo"
author: "Josep R.C."
date: "12/4/2020"
output: html_document
---
## Carga de datos


Enlace  a estos datos  de [Netflix](https://www.kaggle.com/netflix-inc/netflix-prize-netflix)
Generad  un proyecto nuevo. Bajad lo datos de netflix a un carpeta/directorio que se llame `netflix`  y dentro de `netflix` crear una carpeta/directorio que se llame `model_netflix`



Sabemos que en combined combined_data_1.txt hay 2342 películas y tiene 12095343. Cada pelicula está separada por un entero por ejmeplo  `1:`  eas de cir un entero seguido de `:`.


Si queremos leer una cuántas películas tenemos que leer   sólo algunas lineas . Por ejemplo para leer las 100 primeras películas tenemos que leer las lineas hasta en encontrar la película 101 es decir `352872` lineas.


Película Núm.  |  ID_película | fila
---------------|--------------|---------
   1           |    1:        |        1
 101           |  101:        |   352872
 201           |  201:        |   934086
 301           |  301:        |  1454270
 501           |  501:        |  2799205
1001           | 1001:        |  5011200
2001           | 2001:        | 10319270


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=TRUE)
library(tidyverse)
```

## Carga de datos

```{r}
#cargamos la librería tidyverse... lo normal es cargarlo oculto en el setup
library(tidyverse)
#cargamos los datos de combined netflix
n_max=352872-1 # leo las primeras 100 películas 
# n_max=Inf leo todas
# con este límite cargamos hasta la película que hace 1000 del combined_data_1.txt, para cargar todas poned n_max_1=Inf para cargar #todas la  filas del fichero  son +24058263
netflix=read_tsv("data/combined_data_1.txt",n_max = n_max,col_names = FALSE)
dim(netflix) 
# si n_max=Inf hemos cargado 12097685 lineas unos 12 millones
# los cuatro ficheros .txt tienen en total unos 100 millones de líneas
head(netflix)
object.size(netflix)
```
Arreglamos los datos

```{r}
netflix=netflix %>% mutate(fila=row_number())
filas=grep(":",netflix$X1) #grep identifica las rows en las que hay el caracter ':' en este caso
#save(filas,file="data/filas_1.Robj")
filas_ID= netflix %>%
  filter( fila %in% filas ) %>%                #dice donde empieza cada pelicula.
  mutate(ID=as.integer(gsub(":","",X1)))      #gsub sustituye el ":" por nada "" de la columna X1
# IDs=unique(filas_ID$X1) # no lo usa para nada pero bueno 
reps=diff(c(filas_ID$fila,max(netflix$fila)+1)) #devuelve el numero de rows de cada peliID que al final es el número de usuarios que han puntuado cada peli.
```

```{r}
netflix=netflix %>%
  mutate(ID1=rep(filas_ID$X1,times=reps)) %>%
  filter(!(fila %in% filas)) %>%
  select(-fila) %>%
  separate(X1,into=c("ID_user","Score","data"),sep=",") %>%
  mutate(Score=as.integer(Score)) %>%
  separate(col = ID1,into=c("ID_film","borrar")) %>%
  select(-borrar) %>% mutate(ID_film=as.numeric(ID_film))
```

```{r}
glimpse(netflix)
class(netflix)
ncol(netflix)
nrow(netflix)
```

```{r}
length(unique(netflix$ID_user))
```
Mejorar las siguientes estadísticas. 

```{r}
table(netflix$ID_user) -> count_users
table(netflix$ID_film) -> count_films

```

```{r}
plot(sort(table(count_users)))
plot(sort(table(count_films)))
```
```{r}
movie_titles <- read_csv("Data/movie_titles.csv")
names(movie_titles)=c('ID_film', 'Year', 'Title')
which(count_films==max(count_films))
filter(movie_titles, ID_film == 30)
```

## Similaridades entre películas

la similaridad cosen de dos vectores de $\mathbb{R}^n$ no nulos $A=(A_1,A_2,\dots,A_n)$ y $B=(B_1,B_2,\dots,B_n)$

```{r}
sim_cos_netflix=function(xy,data=netflix){
  #data=netflix; xy=c(1,2)
  x=xy[1]
  y=xy[2]
  x=1
  y=2
  x1=filter(data,ID_film==x)
  y1=filter(data,ID_film==y)
  xy=inner_join(x1,y1,by='ID_user')
  sim=sum(xy$Score.x*xy$Score.y)/sqrt(sum(x1$Score^2)*sum(y1$Score^2))
  sim
}
```

```{r}
sim_cos_netflix(c(1,2),netflix)
sim_cos_netflix(c(2,1),netflix)
sim_cos_netflix(c(1,1),netflix)
```

```{r}
choose(100,2) #cuantas combinaciones de 2 se pueden hacer con 100 películas
# Calculamos la lista de todas las similitudes que vamos a caluclar
aux=t(combn(unique(netflix$ID_film),m=2))
sim=tibble(x=aux[,1],y=aux[,2])


time_sim <- system.time(sim$sim <- as.numeric(apply(sim,1,sim_cos_netflix)))
time_sim
```

```{r}
#install.packages('reshape2')
library(reshape2) #libreria que reformatea datos
#añado la diagonal con similitud = 1
diag_sim = tibble(x=1:100,y=1:100,sim=1)
#construyo la matriz solo la parte triangular superior
acast(rbind(sim,diag_sim), x~y, value.var = 'sim') -> Sim_cos_matrix1
#Sustituimos los NA por 0
sim_cos_matrix1[is.na(sim_cos_matrix1)]=0
sim_cos_matrix1 = sim_cos_matrix1 + t(sim_cos_matrix1) #completo la matriz de similitud 
diag(sim_cos_matrix1)=1
```

## Similitud en paralelo



```{r}
Sim_cos_matrix2=matrix(0,ncol=100,nrow=100)
```


```{r}
#install.packages(“parallel”)
#iInstall.packages(“doParallel”)
#library(doParallel)
library(parallel)

# Detect the number of available cores and create cluster
no_cores <- parallel::makeCluster(detectCores())
no_cores
length(no_cores)
#str(no_cores)
cl <- makeCluster(length(no_cores)-1)  
#registerDoParallel(cl)

pares=tibble(xy=t(combn(unique(netflix$ID_film),m=2))) %>% transmute(x=as.numeric(xy[,1]),y=as.numeric(xy[,2]))

clusterCall(cl, function() library(tidyverse))
clusterExport(cl,list("sim_cos_netflix","Sim_cos_matrix2","netflix","pares"))

# Run paralell 1
# Run parallel computation
t1=Sys.time()
time_sim_parallel <- system.time(
results<-parApply(cl,
                  pares,1,
                  FUN=function(x) {c(x[1],x[2],sim_cos_netflix(x,data=netflix))})
)

apply(results,2,FUN=function(x) {Sim_cos_matrix2[x[1],x[2]]<<-x[3]})
# arreglo la triangular inferior 
Sim_cos_matrix2=Sim_cos_matrix2+t(Sim_cos_matrix2)
diag(Sim_cos_matrix2)=1#arreglo la diagonal
all(Sim_cos_matrix2==Sim_cos_matrix1)
t2=Sys.time()
t2-t1
time_sim_parallel 
# Close cluster
parallel::stopCluster(cl)
time_sim_parallel
time_sim
```



## CLustering jerarquico,  y proyección MDS

Hace un clustering de las distancias

```{r}
hclust(as.dist(1-Sim_cos_matrix2),method = "average")-> h

plot(h,hang = -1)
```



```{r}
h_d=as.dendrogram(h)
plot(h_d)
```



cmdscale, escala distancias
```{r}
mds=cmdscale(1-Sim_cos_matrix2)
plot(mds)
kmeans=kmeans(mds,3)
mds=as.data.frame(mds)
mds$cluster=as.factor(kmeans$cluster)
```



Lo siguiente dice que no es de esta asignatura

```{r}
# #install.packages("ggpubr")
# library(ggpubr)
# my_tibble = tibble(row_name = letters[1:10], a = 1:10, b = 11:20, c = 21:30)
# my_tibble
# row_name = my_tibble$row_name
# my_tibble %<>% select(-row_name) %>% as.matrix
# rownames(my_tibble) = row_name
```


## Trabajo

- El data wrangling que corrige ricardo es lo que hemos hecho hoy al inicio más o menos juntando las 100 pelis de cada archivo y hacer los joints. 

- Lo que hemos hecho hoy sería la parte descriptiva que lo corrige JB
100 de cada fichero, calcular estadisticos por fechas, mirar findes, que dia puntuan menos, media de puntuaciones de pelicula por semana por dia (o incluso por serie temporal, como han ido cambiando la puntuacion de cada peli).
El primer fichero tiene 4499 peliculas. Algo más complicado seria que scaramos las peliculas mas puntuadas(Sonia ha cortado a Ric asi que nada xd). No hace falta hacer similitud, solo analisis descriptivo por fechas, semana, mes, dia de la semana (así podemos agrupar cuando se puntua) y por pelicula. 

### TO DO:

- Wrangling:
  - juntar las 400 pelis
  - join
- Descriptiva:
  - Mejorar las siguientes estadísticas. 
    - Hacer estadisticas por fecha tambien
    - Meter ranking de las peliculas mas votadas y mejor votadas
    - Distribucion de los scores (boxplot,barplot)
    - Cunatos usuarios valoran una peli, cuantos dos, ...
    - 

  


Preguntas que le podemos hacer a los datos:



Cluster: como si fueran tantos ordenadores como cores que pueden trabajar en paralelo
Luego, a cada "pc" le diremos que tiene que hacer. Se le tinene que pasar las librerias y los datos a usar por los clusters

Cerrar R y cancelar aplicaciones de R si aun quedan cosas, ir Task Manager para ver lo que usa más memoria. En Resource Monitore podemos ver el uso de los cores